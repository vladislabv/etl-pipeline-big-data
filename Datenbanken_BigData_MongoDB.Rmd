---
title: "Datenbanken in BigData: Team MongoDB"
abstract: ""
keywords: "MongoDB, Python"

course: Datenbanken in Big Data (Prof. Dr. Buchwitz)
supervisor: Marius ...
city: Meschede

# List of Authors
author:
- familyname: Stasenko
  othernames: Vladislav
  address: "MatNr: 30345058"
  qualifications: "Data Science (MA, 2. Semester)"
  email: stasenko.vladislav@fh-swf.de

- familyname: Ulbrich
  othernames: Patrick Adrian
  address: "MatNr: 12345679"
  qualifications: "Data Science (MA, 2. Semester)"
  email: ulbrich.patrick@fh-swf.de
  
- familyname: Metzner
  othernames: Hendrik
  address: "MatNr: 12345679"
  qualifications: "Data Science (MA, 2. Semester)"
  email: metzner.hendrik@fh-swf.de

- familyname: Fenske
  othernames: Marvin
  address: "MatNr: 10058886"
  qualifications: "Data Science (MA, 2. Semester)"
  email: fenske.marvin@fh-swf.de

- familyname: Eker
  othernames: Sinan
  address: "MatNr: 12345679"
  qualifications: "Data Science (MA, 2. Semester)"
  email: eker.sinan@fh-swf.de

- familyname: Alhelal
  othernames: Omar
  address: "MatNr: 30355303"
  qualifications: "Data Science (MA, 2. Semester)"
  email: alhelal.omar@fh-swf.de

# Language Options
german: true # German Dummy Text
lang: de-de   # Text Language: en-gb, en-us, de-de

# Indexes
toc: true     # Table of Contents
lot: false    # List of Tables
lof: false    # List of Figures

# Output Options
bibliography: references.bib
biblio-style: authoryear-comp
blind: false
cover: true
checklist: false
output:
  fhswf::seminarpaper:
    fig_caption: yes
    fig_height: 5
    fig_width: 8
    keep_tex: no
    number_sections: yes
    citation_package: biblatex
knit: fhswf::render_seminarpaper
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache=FALSE, messages=FALSE, warning=FALSE, 
                      attr.source='.numberLines', singlespacing = TRUE)
fhswf::fhswf_hooks()

# Load Packages
library(fhswf)
library(ggplot2)
```

# Einleitung

# Einführung in MongoDB

MongoDB ist eine Open-Source-NoSQL-Datenbank, die auf nichtrelationalen Prinzipien aufbaut. Diese Datenbank ist flexibel und kann sowohl strukturierte, als auch unstrukturierte Daten verarbeiten. Sie ist dokumentenorientiert und in einer unstrukturierten Query Language implementiert.

Die Besonderheit von MongoDB liegt in der hohen Flexibilität. Das System ermöglicht das Speichern und Verarbeiten von Daten in verschiedenen Formaten. Im Gegensatz zu den herkömmlichen relationalen Datenbanken kann MongoDB auch größere Datenmengen problemlos verarbeiten.

Ein weiteres Merkmal von MongoDB ist das BSON-Format (Binary JSON), eine binäre Variante von JSON (JavaScript Object Notation). BSON bietet einen erweiterten Bereich von Datentypen an und ist damit besonders vielseitig.[@PureStorage]

## Datenbankstruktur

Im Gegensatz zu den üblichen relationalen SQL-Datenbanken arbeitet MongoDB zur Datenspeicherung nicht mit Tabellen und Spalten, sondern mit Collections und Documents.

Documents in MongoDB bestehen aus Wert-Schlüssel-Paaren und bilden die Grundlage für die Datenspeicherung. Darüber hinaus beinhaltet MongoDB Collections, die wiederum diese Documents speichern. Jedes dieser Dokumente ist einzigartig und kann eine beliebige Anzahl von Feldern enthalten. Der Aufbau eines Dokuments wird durch den Aufbau der Klassen und Objekte bestimmt, welche vom Entwickler in der verwendeten Programmiersprache definiert werden. MongoDB unterstützt mehrere Programmiersprachen, wie z. B. C, C++, C#, Java, Python, Ruby und Swift. [@DataScientest]

## Funktion und Nutzen

MongoDB bietet Organisationen eine umfangreiche Auswahl an Einsatzmöglichkeiten:

**Datenspeicherung:** MongoDB ist hochflexibel und kann sowohl große strukturierte als auch unstrukturierte Datensätze verarbeiten. Die Skalierbarkeit der Datenbank erstreckt sich auf vertikale und horizontale Ebenen. Abfragen können nach Feldern, Bereichen und Ausdrücken erfolgen.

**Komplexe Datenstrukturen**: Mit MongoDB lassen sich komplexe Datenstrukturen darstellen. Das Dokumentenmodell unterstützt die Verschachtelung von Dokumenten, was vor allem bei der Abbildung hierarchischer Strukturen hilfreich sein kann. Darüber hinaus lassen sich mit MongoDB variable Datenstrukturen leicht abbilden.

**Lastenausgleich**: MongoDB kann auf mehreren Servern ausgeführt werden, womit eine dynamische Lastverteilung möglich ist. Dadurch wird die Verfügbarkeit der Datenbank erhöht, was vor allem in Umgebungen mit erhöhtem Datenverkehr und hohem Arbeitsaufkommen von Nutzen ist.

**Datenintegration**: MongoDB eignet sich ideal für die Integration von Daten in Anwendungen, einschließlich Hybrid- und Multi-Cloud-Anwendungen. Sie können Daten aus unterschiedlichen Quellen zusammenführen.

Diese Anpassungsfähigkeit und Skalierbarkeit machen MongoDB zu einer idealen Lösung für Organisationen, die komplexe Datenstrukturen speichern, verarbeiten und verwalten müssen. [@Alexander]

## Datenbeschreibung

# Anwendungsfall

Im Rahmen dieser Projektarbeit werden die von den Ruuvy Tags gelieferten Daten im Umfeld eines Supermarkts betrachtet. Gateways sind dabei über verschiedene Abteilungen (z.B. die Frischfleisch-, Obst- & Gemüse-, Tiefkühl- oder Aktionswarenabteilung) verteilt. Die von den Tags gesendeten Werte werden dabei als Werte der entsprechenden Abteilung gewertet. Die Ruuvy Tags werden an allen Einkaufswagen angebracht. Sie liefern Daten über die Beschleunigung der Einkaufswagen, die Temperatur, Luftfeuchtigkeit und den Luftdruck in der Abteilung sowie den Ladezustand der eigenen Batterie. Die Beschleunigung der Einkaufswagen kann als Indikator für die Bewegung eines Einkaufswagens gewertet werden da Menschen in der Regel nicht in der Lage sind, ein Objekt mit einer genau gleichbleibenden Geschwindigkeit (also ohne Beschleunigung) zu bewegen. Eine Beschleunigung von 0 m/s\^2 bedeutet demnach, dass der Einkaufswagen steht. Zielsetzung im Rahmen dieses Projektes ist es, anhand der Beschleunigungsdaten der Einkaufswägen zu erkennen, welche Abteilungen im Markt viele Kunden anziehen. Dabei wird ein häufiges Beschleunigen innerhalb einer Abteilung eines Wagens als Interesse an mehreren Produkten betrachtet. Gibt es viele verschiedene Tags, die sich in einer Abteilung parallel aufhalten ist diese vermutlich stark frequentiert und von Interesse für die Kunden. Die Produkte müssten entsprechend häufiger nachgefüllt werden. Gibt es bei Aktionspreisen Schwankungen zu einem regulären Verhalten können kurzfristig zudem Aussagen über den Erfolg des Angebotes erhalten werden. Zudem können Informationen darüber, wie schnell ein Tag die Gateways wechselt, für die Planung zeigen, wie lange sich Kunden in den verschiedenen Abteilungen aufhalten. Eine Auswertung anderer Daten des Ruuvy Tags wie die Temperatur wäre denkbar, ist aber nicht Teil dieses Projekts.

# Datenbankstuktur

Um die Datenbank mit MongoDB zu erstellen wurde zunächst der Use Case betrachtet. Im Zentrum der Abfrage stehen dabei immer die Gateways (Abteilungen des Supermarktes). Tags können dabei zwischen den Gateways wechseln und an verschiedenen Tags unterschiedliche Messergebnisse liefern. Aus diesem Grund wurde sich hier für eine Struktur mit vier Tabellen entschieden. Diese sind die Tabellen gateways, tags, measures und configs. In gateways befinden sich Informationen über die einzelnen Gateways (wie beispielsweise die IP Addresse und das Datum der letzten Verbindung). Außerdem beinhaltet diese Tabelle jeweils ein Array, in dem sich die Tag ID's befinden, die ihm zugeordnet sind sowie der eines mit den Config ID's. Sie verweisen als Fremdschlüssel auf die Tabellen tags und configs. Es wurde sich dagegen entschieden, diese Tabellen in Gateways zu embedden. Grund dafür ist die ansteigende Speichergröße. Im Rahmen dieses Projekts wird nur eine kostenlose Version von MongoDB über MongoDB Atlas verwendet. Da das Einbetten der Tabellen deren Größe stark wachsen lässt wurde sich entschieden, auf eine bessere Abfragegeschwindigkeit zugunsten des Speichers zu verzichten. Zusätzlich wird die Tabelle configs verwendet, um sowohl für die Gateways als auch für die Tags unterschiedliche Konfigurationsdaten zu speichern. Diese müsste demnach alternativ in beide Tabellen (gateways und tags) embedded werden. Innerhalb der Tabelle tags befinden sich Informationen über die einzelnen Tags. Dazu zählt die ID, der Name, das Datum der letzten Verbindung, ein Flag, das zu erkennen gibt, ob der Tag online ist sowie die KonfigurationsID. Zuletzt werden die Messwerte der Einkaufswagen mit eigener ID, einem Timestamp und der Tag ID als Fremdschlüssel in measures gespeichert.

```{r database, echo = FALSE, message=FALSE, fig.align='center', fig.cap='Datenbankstruktur', out.width='1\\linewidth', fig.pos='H'}
knitr::include_graphics("./fig/Struktur.png")
```

Die obere Architektur ermöglicht einfache Top Down Abfragen. Die Gateways sind dabei immer Ziel der Abfragen und können per Query, je nach Bedarf, runter auf Tag- oder Measures Ebene. Beispiele aus dem Use Case für eine Abfrage auf Tag Ebene währe dabei, wie viele Einkaufswagen sich in einer Abteilung befinden. Auf Measures Ebene stände die Frage, wie oft Einkaufswagen innerhalb einer Abteilung beschleunigt werden.

# Python-Applikation

Bevor man ins Tiefe geht, muss folgendes erwähnt werden:

* Um sensible Daten zu schützen, wie z.B. Zugangsdaten für eine Datenbank, werden diese in einer separaten Datei gespeichert und per `python-dotenv` package aufgerufen. Dabei kommen solche Funktionen, wie `os.getenv()` oder `os.environ.get()` zur Hilfe.

* Dadurch die Ruuvi-API eine REST-API Architektur nutzt, wurde es entschieden, für GET-Abfragen eine python-Package `requests` zu verwenden.

* Die Applikation wurde als einen Python Module geschrieben, so kann es sehr leich in einen größeren Projekt integriert werden.

* Die Daten werden syncron (bzw. sequentiell) in eine MongoDB (shared) Cluster hochgeladen, die eine Speicherbegrenzung von 512 MBs hat, und zwar mithilfe der `pymongo` python-Package.

* Das Neustarten von der Python-Applikation bereinigt die Datenbank mit allen bevor gesammelten Daten.

Der ganze Modul besteht aus folgende Python-Dateien:

**api.py**

Hier werden die ganzen Funktonen definiert, die zur Datenextrahierung aus der API benutzt werden.

Dabei müssen die wichtigsten Annahmen bzw. Methoden zur Datenextrhierung erwähnt werden:

1. MongoDB kann leider keine Nanosekunden, deswegen werden diese abgeschnitten, um einen String-Timestamp in einen DateTime-Timestamp überführen zu können. Das betrifft Felder "last_contact" bei Tags und Gateways, und "recorded_time" bei Measurements.

2. Bei der Pagination über Measurements muss sichergestellt werden, dass man bei einem Tag nicht x-mal über die gleiche Seite iteriert, dafür müssen die Checkpoints zwichengespeichert werden. Bei der Pagination wird den Progress in die Measurements-Dokumenten mitgeschrieben, was in Zusammenhang mit einer zugehörigen Tag-Adresse eine robuste Methode zur Speicherung der Checkpoints anbietet.

3. Bei der Pagination wird eine Annahme getroffen, dass es unendlich iteriert wird, bis man eine auf eine Seite mit Measurements stößt, wo es weniger als der zugelassene Maximum angezeigt wird (< 10).

**dataloader.py**

Es wird grundsätzlich eine While-Schleife wird verwendet, um alle historische Änderungen von Gateways und Tags zu tracken.

1. Während eines While-Zykles wird zuerst den Endpoint "/structure/gateway/list" abgefragt, so bekommt man eine Liste aus Gateways, die dann weiterverarbeitet werden.
2. Für jeden Gateway wird es iteriert und dabei pro Iteration werden zwei Tasks erledigt

  1. Es wird zuerst einen Python Dictionary verformt, der 1 zu 1, wie in der Datenbank-Schema für MongoDB aussieht.
  2. Für jeden gateway ID wird eine weiter API GET-Abfrage verformt, die eine Liste aus den zugehörigen Tags zurückgibt.

3. Es kommt zu einer weiteren For-Scheife für Tags, diese werden dann zuerst für MongoDB Import vorbereitet wie vorher die Gateways. Hier wird werden die Tag-Addressen in eine Liste hinzugefügt, die endlich bei einem Gateway in der MongoDB Collection auftaucht. Bevor den Tag endlich in die Collection gespeichert wird, wird es auf die Funktion `updateInsert()` zugegriffen, um die Dopplungen beim Speichern zu vermeiden.

4. Dann gibt es die 3. Stufe von der For-Schleife, wobei es eine GET-Abfrage für Measurements zusammengestellt wird. Die zurückgegebenen Measurements werden für die Speicherung in die Datenbank wie vorher vorbereitet und mithilfe von `updateInsert`-Funktionen in die zugehörige Collection gespeichert.

**update.py**

Beinhaltet alle Hilfsfunktionen, die mit einer MongoDB-Datebank mithilfe von `pymongo`-Package interragieren.

Die Datei beinhaltet verschiedene Funktionen, die aber folgende Funkionalität am Ende erlauben:

* `getLastPageNumber()` -> gibt eine aktuelle Seitennumer für den jeweiligen Tag Id, der ín die Funktion übergeben wurde. Das wird intern benötigt, um die Pagination über Measurements steuern zu können.

* `checkLastTag()` -> prüft ob der übergebene Tag von einem vorher gespeicherten Tag in der Collection "Tags" unterscheidet. Falls es der Fall ist, wird dieser in die Datenbank gespeichert, falls es keine Änderung für den Tag gab, wird es verworfen, somit werden Duplikaten vermeidet.

* `updateInsert()` -> eine Funktion, die unmitterlbar einen übergebenen Dictionary in eine gegebene Collection speichert.

# Datenabfrage

Wie Kommen eigentlich die Daten aus der Datenbank?
Um mit MongoDB zu interagieren, haben wir uns sichergestellt, dass die erforderlichen Bibliotheken Pymongo installiert ist. **PyMongo** ist das offizielle Treiber-Paket, um MongoDB mit Python anzusteuern.

``` python
import pymongo
```

Danach stellten wir eine Verbindung zur MongoDB-Datenbank her.

``` python
client = pymongo.MongoClient("mongodb://localhost:27017/")
db = client['MongoDB-Database']
```

In MongoDB werden die Daten in sogenannten "Collections" (Sammlungen) organisiert und gespeichert. Eine Collection ist eine Gruppe von Dokumenten, die ähnliche oder verwandte Daten enthalten. In unserem Fall sind es drei Sammlungen

``` python
gateways_collection = db["measures"]
measures_collection = db["gateway"]
tags_collection = db["tags"]
```

Um sie abzufragen:

``` python
documents_to_find = {}
one_document = gateways_collection.find_one(documents_to_find)
print(one_document)
```

Nachdem wir gesehen haben wie die Verbindung und Integration zwischen MongoDB und Python funktioniert, stellen wir vier verschiedene Abfragebeispiele in Beuzg auf Use Case zum Verständnis dieser Anwendung und ihrer Funktionsweise. Sie dienen als klare Referenz für die Verwendung von Abfragen und können dazu beitragen, Missverständnisse oder potenzielle Fehler in der Anwendung zu vermeiden. Dabei werden nicht nur Abfragebeispiele dargestellt sondern auch visualisiert

Am Anfang defenieren wir die Aggregation-Stages als Zeichenkette(Strings), da die Daten in MongoDB in BSON-Format gespeichert sind.

::: {.alert .alert-block .alert-success}
<b>Zur Info:</b> BSON ist ein binäres, JSON-ähnliches Format, das zur Speicherung strukturierter Daten verwendet wird. Im Gegensatz zu JSON unterstützt BSON jedoch einige zusätzliche Datentypen und Features, die für die Speicherung in einer MongoDB nützlich sind.
:::

Sobald wir alle Stages haben, konstruieren und führen wir die Aggregationsabfrage aus.

\newpage

# Datenvisualisierung

Im Rahmen des festgelegten Use Cases wurden 4 zu beantwortende Fragen definiert. Alle 4 Fragen werden mit entsprechenden Abbildungen dargestellt. Der Code zum Erzeugen der Darstellungen und die entsprechenden Datenbankabfragen sind in gleicher Reihenfolge als Anhang im Ordner *src* im Notebook *mongodb_queries.ipynb* zu finden.

Die erste Abfrage lautete: *In welcher Abteilung halten sich die meisten Kunden auf?* In Bezug auf den Anwendungsfall wurde entsprechend ausgewertet, wie viele Tags mit einem Gateway im Stand der letzten Messung verbunden sind. Die Visualisierung ist in der Darstellung \@ref(fig:most-customers) zu finden. Es ist zu erkennen, dass die Tags nicht gleichverteilt mit den Gateways verbunden sind. Insbesondere fällt ein Gateway auf, welches nur mit einem einzigen Tag zum Zeitpunkt der letzten Messung in Kontakt stand.

```{r most-customers, echo = FALSE, message=FALSE, fig.align='center', fig.cap='Momentaufnahme Kunden je Abteilung', out.width='1\\linewidth', fig.pos='H'}
knitr::include_graphics("./fig/most_customers_now.pdf")
```

Als Filialmarkt ist ebenfalls das Design der Abteilungen interessant. Daher lautete die zweite Abfrage: *In welchen Abteilungen gibt es die meisten Zusammenstöße mit Regalen?* Im Rahmen der gegebenen Daten wurde eine Beschleunigung in X-Richtung von über 200 m/s², was etwas mehr als 20G entspricht, als Zusammenstoß festgelegt. Anders als zur Beantwortung der ersten Frage, wurde zur Beantwortung dieser Frage die Darstellung zusätzlich auf die höchsten 5 Werte entsprechend der Anzahl der relevanten Events eingeschränkt. Die Ergebnisse sind in Darstellung \@ref(fig:most-crashes) dargestellt.

```{r most-crashes, echo = FALSE, message=FALSE, fig.align='center', fig.cap='Zusammenstöße mit Regalen je Abteilung', out.width='1\\linewidth', fig.pos='H'}
knitr::include_graphics("./fig/most_crashes.pdf")
```

Eine weitere wirtschaftliche Frage für einen Filialmarkt ist die Frage nach den Abteilungen mit dem höchsten Kundeninteresse. Kundeninteresse für Produkte bedeutet im Anwendungsfall, dass ein Kunde ein Produkt in einer Abteilung betrachtet und gegebenenfalls kauft. Der Kunde muss somit Anhalten, ein Produkt betrachten und gegebenenfalls in den Einkaufswagen legen, und im Anschluss Weitergehen. Mit diesem Stoppen und Starten gehen somit höhere Beschleunigungswerte einher. Da die Tags, die sich an den Einkaufswagen befinden, die Beschleunigung in X-Richtung messen, wurden die durchschnittlichen Beschleunigungswerte je Gateway als Näherung für das Kundeninteresse in einer Abteilung gewertet. Es werden in Abbildung \@ref(fig:highest-interest) ebenfalls die 5 höchsten Werte dargestellt.

```{r highest-interest, echo = FALSE, message=FALSE, fig.align='center', fig.cap='Abteilungen mit dem höchsten Kundeninteresse', out.width='1\\linewidth', fig.pos='H'}
knitr::include_graphics("./fig/highest_interest.pdf")
```

Wie in der Darstellung \@ref(fig:highest-interest) zu erkennen, hat die Visualisierung leider keine direkte Aussagekraft. Die Datengenerierung wurde daraufhin genauer untersucht und die Darstellung scheint dem Datengenerierungs-Schema geschuldet zu sein. Es konnten lediglich 200 distinkte Messwerte für den Wert *acc_x* gefunden werden. Alle Werte kamen dabei gleich häufig vor. Die unterschiedliche Anzahl an Messwerten über 200 m/s² je Gateway, wie in Abbildung \@ref(fig:most-crashes) dargestellt, stammt lediglich daraus, dass diese Gateways mehr der 200-Messwerte-Zyklen durchlaufen haben. Im Durchschnitt sind durch diese Datengenerierung aber die Durchschnittswerte für alle Gateways identisch. Eine tiefere Untersuchung der Datengenerierung ist ebenfalls im Notebook *mongodb_queries.ipynb* unter der Überschrift *Untersuchung der Visualisierung von Abfrage 3* zu finden.

Die letzte aufgestellte Frage zur Einschätzung der Abteilungs-Performance war: *Wie lange verbringt ein Kunde durchschnittlich in einer Abteilung?* Zu Beantwortung dieser Frage wurde zunächst die Verbindungszeit eines Tags mit einem Gateway berechnet. Im Anschluss konnte aus diesen Ergebnissen die durchschnittliche Verbindungszeit je Gateway ermittelt werden. Für diese Visualisierung wurden neben den 5 Abteilungen mit dem höchsten Kundeninteresse auch die 5 Abteilungen mit dem niedrigsten Kundeninteresse ausgegeben. Das Ergebnis ist in Darstellung \@ref(fig:avg-time-gateway) zu finden.

```{r avg-time-gateway, echo = FALSE, message=FALSE, fig.align='center', fig.cap='Abteilungen mit höchster und niedrigster Durchschnittszeit', out.width='1\\linewidth', fig.pos='H'}
knitr::include_graphics("./fig/avg_time_gateway.pdf")
```

In der Darstellung \@ref(fig:avg-time-gateway) ist ebenfalls die Datengenerierung die Ursache für die Darstellung. Alle Gateway-Tag-Verbindungen haben annähernd identische Werte in der Verbindungszeit, womit die Betrachtung des Durchschnittswerts erneut beinahe identische Werte, bis auf Abweichungen im Millisekundenbreich, für alle Gateways liefert.

\newpage

# Fazit

## Challenges
Während der Entwicklung ergaben sich verschiedene Herausforderungen. Auf diese Herausforderungen, die resultierende Lösung und deren Vorteile wird im folgenden Abschnitt eingegangen.

### Zyklische Abfrage der Daten
Ziel des Programms sollte es sein, dass Messwerte nicht verloren gehen. Es konnte jedoch zu Beginn der Entwicklung nicht abgefragt werden, welches der letzte gespeicherte Messwert zu einem bestimmten Tag war, da die Messwerte nicht zeitlich fortlaufend hintereinander geschrieben wurden. Es gab also keinen Anhaltspunkt, wie viele Messwerte bereits eingelesen wurden. Aufgrund der Masse an Daten konnten bei einem Neustart auch nicht alle Messwerte neu abgefragt und gespeichert werden. Als Lösungsansatz wurde gewählt, dass die Seite, auf der ein Messwert erfasst wurde, mit in der Datenbank hinterlegt wird. Bei der Abfrage der Messwerte zu einem bestimmten Tag wird als erstes die höchste bereits eingelesene Seite aus der Datenbank abgefragt und anschließend die nächsten 50 Seiten eingelesen. Die Begrenzung auf 50 Seiten pro Tag wurde gewählt, damit die Tags in einer höheren Frequenz durchlaufen werden und so eine eventuelle Änderung der Gateways zu den Tags öfter abgefragt werden kann. Des Weiteren wird nur jede volle Seite importiert, damit keine Messwerte doppelt erfasst werden. Da die letzte Seite mit Messwerten zu jedem Tag mit abgespeichert wird, kann das Programm als Cron-Job genutzt werden und so in regelmäßigen Abständen die Daten in die Datenbank schreiben.

### Historische Zuordnung der Tags zu den Gateways {#historical}
In der ersten Version des Programms wurde immer nur der aktuelle Zustand des Tags gespeichert. Das bedeutet, dass die Zuordnung der Tags zu den Gateways mit jedem Import überschrieben wurde. Es war außerdem so nicht möglich eine Historie über den Verlauf der Tags über die verschiedenen Gateways oder die Konfigurationen in zeitlichem Verlauf zu speichern. Dafür wurden zwei verschiedene Lösungsarten gefunden. Zum einen kann eine Historie der Tags angelegt werden. Dies wird erreicht indem der Importzeitpunkt des Tags in die Datenbank gespeichert wird. Mit diesen wurden auch die jeweiligen Einstellungen zu den Tags gespeichert. Über eine Sortierung der Tags nach dem Einfüge-Datum kann der Verlauf verfolgt werden. Die weitere Lösung bestand darin die Gateway-ID und die Tag Adresse zu jedem Messwert zu speichern. Dafür wurde eine Kombination aus beiden Ansätzen gewählt. So werden einerseits die historischen Einstellungen der Tags mit einem Einfüge-Datum gespeichert, als auch die Abfrage der Zuordnung der Tags zu Gateways beschleunigt. 

### Nanosekunden in der API
Da die API in der Programmiersprache Go programmiert wurde, werden Timestamps der API mit Nanosekunden zurückgegeben. MongoDB nutzt zur Speicherung des Timestamp Datentyps 64 Bit [@MongoBSON]. Es ist also nur möglich die Microsekunden mit abzuspeichern. Als Lösungsweg ergaben sich zwei Ansätze. Zu einen ergab sich die Möglichkeit den Timestamp als String in der Datenbank abzulegen, oder zum anderen die Nanosekunden vor dem Cast vom String in einen Timestamp abzuschneiden. Da die Nanosekunden nicht unbedingt zum Erreichen unseres Anwendungsfalles nötig sind, wurde die zweite Möglichkeit umgesetzt, um einerseits Speicherplatz zu sparen und andererseits die Abfragegeschwindigkeit zu erhöhen.

\newpage

## Lessons Learned and Future Changes
Wie bereits in [Challenges](#historical) erwähnt, änderte sich während der Entwicklung mehrfach der Anwendungsfall oder die Voraussetzungen an das Programm. Dieses zog eine Änderung der Datenbankstruktur, der Validation und der Indexe nach sich. Es sollte für zukünftige Projekte mit dokumentenorientierten Datenbanken bei der Planung der Struktur der Anwendungsfall im Vordergrund stehen und weniger die Daten an sich wie etwa bei relationalen Datenbanken. Daten, die zusammen abgefragt werden, sollten auch zusammen gespeichert werden.
Des Weiteren sollte sich bei der Planung genau mit der Art der Daten auseinandergesetzt werden. Im Falle der Messwerte konnte der Vorteil der von MongoDB bereitgestellten Time-Series Collection genutzt werden. Mit dieser Art Collection ließ sich Speicherplatz sparen und ebenfalls die Abfragen beschleunigen. Es ist jedoch wichtig zu erwähnen, dass unser Programm laut [@MongoTimeSeries]  hätte beschleunigt werden können, wenn wir von der insert_one Methode in pymongo auf eine Batch Verarbeitung mit insert_many gewechselt wären. Der Vorteil hätte durch die seitenweise Speicherung der Messwerte genutzt werden können.


\newpage

# Appendix

**mongodb_queries.ipynb**

Dieses Jupyter Notebook enthält die Abfragen zu den entsprechenden Abfrage-Visualisierungen sowie aufgrund der Darstellung \@ref(fig:highest-interest) und \@ref(fig:avg-time-gateway) eine Untersuchung der Datengenerierung.

\newpage

# Technical Appendix {.unnumbered}

```{r, echo = TRUE}
Sys.time()
sessionInfo()
```
